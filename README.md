# recursive llm harness


**Recursive Language Model (RLM)**
you should focus on the core architectural requirements identified in the sources: a persistent environment, programmatic tools to "peek" at context, and a loop for recursive sub-calls [1-3].Below is a structured prompt you can use to generate this shell script harness.### Recommended Prompt for Generating the Harness

**Role:** Act as a Systems Architect and Bash scripting expert. 

**Objective:** Create a minimal shell script harness that implements a 

**Recursive Language Model (RLM)** loop as described in the Zhang et al. paper. The harness should treat a large text file as an **external environment** rather than a direct prompt input [4-6].**Requirements for the Script:**1.  **Environment Initialization:** Load a large text file (the "context") into a path variable. The root model should never see the full content of this file in its initial prompt [2].2.  **The RLM Loop:** Implement a `while` loop that continues until the model outputs a `FINAL(answer)` or `FINAL_VAR(variable_name)` tag [7, 8].3.  **Tool Definitions:** Provide the LLM with a "toolbox" of shell-based commands to interact with the context file, specifically:    *   **Peeking:** Using `head`, `tail`, or `sed` to view specific lines [9, 10].    *   **Grepping:** Using `grep -E` for regex searches to narrow down relevant information [10, 11].    *   **Partitioning:** A way to split the file into smaller temporary chunks [11].    *   **Sub-calls:** A function `llm_query` that sends a small snippet of text to a cheaper/smaller sub-model instance (e.g., via a `curl` command to an OAI-compatible API) [2, 12].4.  

**State Management:** Maintain a history of "REPL" outputs (the results of the shell commands) so the root model can see the results of its previous "peeks" or "greps" [1, 3].5.  **Execution Logic:** The script must extract blocks of code wrapped in triple backticks from the LLMâ€™s response, execute them in the shell, and feed the `stdout` back into the next iteration of the loop [3, 8].

**Output:** Provide a clean, commented `.sh` file that uses `curl` for the LLM API calls and standard GNU/Linux utilities for context manipulation.---### Key Concepts to Include in Your Harness (Based on the Sources)To ensure the generated script functions as a true RLM, the sources suggest these specific implementation details:*   **Handle Context as a Variable:** In a shell script, your "variable" will likely be a file path. The root model should be prompted with the file's size and structure (e.g., line count) but not the content itself [13, 14].*   

**Decomposition Strategy:** The harness must allow the model to follow a "Partition + Map" strategy, where it breaks the file into chunks and calls the sub-model for each chunk before aggregating the results [11].*   **Termination Mechanism:** The sources emphasize the use of specific tags like `FINAL()` to break the loop once the model is confident in its answer [7, 15].*   **Cost Efficiency:** Using a shell script as a "control plane" allows you to use a very small, cheap model (like GPT-5-mini or Qwen-7B) to do the heavy lifting of scanning the context, which significantly reduces API costs [16, 17].*   **Recursive Depth:** While the script can support any depth, the sources suggest starting with a **depth of 1** (the root model calls a sub-model that does not perform further recursion) to manage complexity [18, 19].
